''' DDPGAgent. '''

# Python imports.
import tensorflow as tf
import numpy as np
import random
import tflearn

# Other imports.
from simple_rl.agents.AgentClass import Agent
from simple_rl.agents.func_approx.ExperienceBuffer import ExperienceBuffer
from simple_rl.agents.func_approx.ActorNetwork import Actor

class DDPGAgent(Agent):

    NAME = "ddpg"

    def __init__(self, obs_dim=None, action_dim=None, action_bound=None, name=NAME, actor_rate=0.0001, critic_rate=0.001, tau=0.001, should_train=True, from_checkpoint=None, gamma=0.99):
        # TODO: Use a shared experience buffer?
        
        Agent.__init__(self, name=name, actions=[])

        assert(type(obs_dim) is int)
        assert(type(action_dim) is int)
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.action_bound = action_bound

        self.gamma = gamma
        
        self.should_train = should_train

        # Fixed parameters
        self.update_freq = 1 # 64
        self.batch_size = 64
        self.should_save, self.save_every = True, 100000
        self.print_loss, self.print_every = True, 10000
        
        self.sess = tf.Session()
        self.actor = Actor(sess=self.sess, obs_dim=self.obs_dim, action_dim=self.action_dim, \
                           action_bound=self.action_bound, learning_rate=actor_rate, tau=tau, batch_size=self.batch_size)

        self.critic = Critic(sess=self.sess, obs_dim=self.obs_dim, action_dim=self.action_dim, \
                             learning_rate=critic_rate, tau=tau)

        self.actor_noise = ActorNoise(mu=np.zeros(self.action_dim))


        self.total_reward = 0
        
        self.reset()

        # Load model from a checkpoint
        if not (from_checkpoint is None):
            self.saver.restore(self.sess, from_checkpoint)
            print('Restored model from checkpoint: {}'.format(from_checkpoint))

    def act(self, state, reward, train=True):
        '''
        Args:
            state (simple_rl.State)
            reward (float)

        Returns:
            (str)
        '''
        # Training
        if self.should_train and self.total_steps > 0 and self.total_steps % self.update_freq == 0 and self.experience_buffer.size() > self.batch_size and train:
            s, a, r, s2, t = self.experience_buffer.sample(self.batch_size)

            target_q = self.critic.predict_target(s2, self.actor.predict_target(s2))
        
            # Compute y-vals
            y = []
            for i in range(self.batch_size):
                if t[i]:
                    # y.append(np.array([r[i]]))
                    y.append(r[i])
                else:
                    # y.append(np.array([r[i] + self.gamma * target_q[i]]))
                    y.append(r[i] + self.gamma * float(target_q[i]))

            # print('y=', y)
            # print('len(y)=', len(y))
            # y_ = np.reshape(y, (self.batch_size, 1))
            # print('y_.shape=', y_.shape)
            # print('len(s)=', len(s))
            # print('s[0].shape=', s[0].shape)
            # print('a=', a)
            predicted_q_value, _ = self.critic.train(s, a, np.reshape(y, (self.batch_size, 1)))

            # self.average_max_q += np.amax(predicted_q_value)

            # print('max_q=', max_q)

            a_outs = self.actor.predict(s)
            grads = self.critic.action_gradients(s, a_outs)
            self.actor.train(s, grads[0])

            self.actor.update_target_network()
            self.critic.update_target_network()


        # TODO: Convert a state to a numpy array so that we can send to the Tensorflow.
        img = state.data # TODO: does it work?
        # print('state.data=', state.data)
        # print('type(state.data)=', type(state.data))

        # action = self.actor.predict(img) + self.actor_noise()
        action_ = self.actor.predict(np.reshape(state.data, (1, self.obs_dim)))

        # TODO: Generate a standard deviation from a neural network.
        
        if train:
            action_ += self.actor_noise()
            
        action = action_[0]

        if not (self.prev_state is None) and not (self.prev_action is None) and train:
            self.experience_buffer.add((np.reshape(self.prev_state, (self.obs_dim,)), np.reshape(self.prev_action, (self.action_dim,)), reward, np.reshape(img, (self.obs_dim,)), state.is_terminal()))

        self.prev_state, self.prev_action = img, action

        # Saving checkpoints (NOTE: We only save checkpoints when training)
        if self.should_train and self.should_save and self.total_steps > 0 and self.total_steps % self.save_every == 0:
            save_path = self.saver.save(self.sess, '/tmp/{}.ckpt'.format(self.name))
            print('At step {}, saved model to {}'.format(self.total_steps, save_path))

        self.curr_step += 1
        self.total_steps += 1

        self.total_reward += reward

        # TODO: Check why this is_terminal is always called on the first step of the episode.
        if state.is_terminal() and self.curr_step > 1:
            # print('terminal state =', state.data)
            print('#Episode=', self.curr_episode, '#steps=', self.curr_step, 'Total_reward=', self.total_reward)
            self.curr_step = 0
            self.curr_episode += 1
            self.total_reward = 0
        # print('action selected =', action)

        return action

    def __str__(self):
        return str(self.name)

    def reset(self):
        
        self.experience_buffer = ExperienceBuffer(buffer_size=1000000)
        self.prev_state, self.prev_action = None, None
        self.curr_step, self.total_steps = 0, 0
        self.curr_episode = 0
        self.saver = tf.train.Saver()
        self.sess.run(tf.global_variables_initializer())

        self.critic.update_target_network()
        self.actor.update_target_network()
        

class Critic(object):
    def __init__(self, sess, obs_dim, action_dim, tau, learning_rate, name='ddpg'):
        # TODO: num_actor_vars?
        self.sess = sess
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.tau = tau

        self.name = name

        self.obs, self.action, self.q_estm = self.critic_network(scope=name + "_critic")

        # TODO: What does trainable_variables retrieve?
        self.network_params = tf.trainable_variables(scope=name + "_critic")

        # print('Critic network_params=', self.network_params)

        self.target_obs, self.target_action, self.target_q_estm = self.critic_network(scope=name + "_critic_target")

        self.target_network_params = tf.trainable_variables(scope=name + "_critic_target")

        # Calling this would update the parameters for the target network.
        self.update_target_params = \
                                     [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \
                                     + tf.multiply(self.target_network_params[i], 1.0 - self.tau)) \
                                     for i in range(len(self.target_network_params))]

        # y_i
        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])
        
        # TODO: Implement weight decay
        self.loss = tflearn.mean_square(self.predicted_q_value, self.q_estm) # + tf.reduce_sum(self.network_params ** 2)

        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)


        self.action_grads = tf.gradients(self.q_estm, self.action)

    def critic_network(self, scope):
        obs = tflearn.input_data(shape=[None, self.obs_dim])
        action = tflearn.input_data(shape=[None, self.action_dim])
        with tf.variable_scope(scope):

            net = tflearn.fully_connected(obs, 400, name='d1', weights_init=tflearn.initializations.truncated_normal(stddev=1.0/float(self.obs_dim)))
            net = tflearn.layers.normalization.batch_normalization(net)
            net = tflearn.activations.relu(net)

            t1 = tflearn.fully_connected(net, 300, name='d2', weights_init=tflearn.initializations.truncated_normal(stddev=1.0/400.0)) # This gives the values from the observation
            t2 = tflearn.fully_connected(action, 300, name='d3', weights_init=tflearn.initializations.truncated_normal(stddev=1.0/self.action_dim)) # This gives the values from the action

            net = tflearn.activation(tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')

            w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
            out = tflearn.fully_connected(net, 1, weights_init=w_init)
        return obs, action, out

    
    # Methods below are the API for the network.     
    def train(self, obs, action, predicted_q_value):
        # print('obs=', obs)
        # print('action=', action)
        # print('predicted_q_value=', predicted_q_value)
        return self.sess.run([self.q_estm, self.optimize], feed_dict={
            self.obs: obs,
            self.action: action,
            self.predicted_q_value: predicted_q_value
        })

    def predict(self, obs, action):
        return self.sess.run(self.q_estm, feed_dict={
            self.obs: obs,
            self.action: action
        })

    def predict_target(self, obs, action):
        return self.sess.run(self.target_q_estm, feed_dict={
            self.target_obs: obs,
            self.target_action: action
        })

    def action_gradients(self, obs, action):
        return self.sess.run(self.action_grads, feed_dict={
            self.obs: obs,
            self.action: action
        })

    def update_target_network(self):
        self.sess.run(self.update_target_params)

# class Critic(object):
#     """
#     Input to the network is the state and action, output is Q(s,a).
#     The action must be obtained from the output of the Actor network.
#     """
# 
#     def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):
#         self.sess = sess
#         self.s_dim = state_dim
#         self.a_dim = action_dim
#         self.learning_rate = learning_rate
#         self.tau = tau
#         self.gamma = gamma
# 
#         # Create the critic network
#         self.inputs, self.action, self.out = self.create_critic_network()
# 
#         self.network_params = tf.trainable_variables()[num_actor_vars:]
# 
#         # Target Network
#         self.target_inputs, self.target_action, self.target_out = self.create_critic_network()
# 
#         self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]
# 
#         # Op for periodically updating target network with online network
#         # weights with regularization
#         self.update_target_network_params = \
#             [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \
#             + tf.multiply(self.target_network_params[i], 1. - self.tau))
#                 for i in range(len(self.target_network_params))]
# 
#         # Network target (y_i)
#         self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])
# 
#         # Define loss and optimization Op
#         self.loss = tflearn.mean_square(self.predicted_q_value, self.out)
#         self.optimize = tf.train.AdamOptimizer(
#             self.learning_rate).minimize(self.loss)
# 
#         # Get the gradient of the net w.r.t. the action.
#         # For each action in the minibatch (i.e., for each x in xs),
#         # this will sum up the gradients of each critic output in the minibatch
#         # w.r.t. that action. Each output is independent of all
#         # actions except for one.
#         self.action_grads = tf.gradients(self.out, self.action)
# 
#     def create_critic_network(self):
#         inputs = tflearn.input_data(shape=[None, self.s_dim])
#         action = tflearn.input_data(shape=[None, self.a_dim])
#         net = tflearn.fully_connected(inputs, 400)
#         net = tflearn.layers.normalization.batch_normalization(net)
#         net = tflearn.activations.relu(net)
# 
#         # Add the action tensor in the 2nd hidden layer
#         # Use two temp layers to get the corresponding weights and biases
#         t1 = tflearn.fully_connected(net, 300)
#         t2 = tflearn.fully_connected(action, 300)
# 
#         net = tflearn.activation(
#             tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
# 
#         # linear layer connected to 1 output representing Q(s,a)
#         # Weights are init to Uniform[-3e-3, 3e-3]
#         w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
#         out = tflearn.fully_connected(net, 1, weights_init=w_init)
#         return inputs, action, out
# 
#     def train(self, inputs, action, predicted_q_value):
#         return self.sess.run([self.out, self.optimize], feed_dict={
#             self.inputs: inputs,
#             self.action: action,
#             self.predicted_q_value: predicted_q_value
#         })
# 
#     def predict(self, inputs, action):
#         return self.sess.run(self.out, feed_dict={
#             self.inputs: inputs,
#             self.action: action
#         })
# 
#     def predict_target(self, inputs, action):
#         return self.sess.run(self.target_out, feed_dict={
#             self.target_inputs: inputs,
#             self.target_action: action
#         })
# 
#     def action_gradients(self, inputs, actions):
#         return self.sess.run(self.action_grads, feed_dict={
#             self.inputs: inputs,
#             self.action: actions
#         })
# 
#     def update_target_network(self):
#         self.sess.run(self.update_target_network_params)

class Actor(object):
    def __init__(self, sess, obs_dim, action_dim, action_bound, learning_rate, tau, batch_size, name='ddpg'):
        self.sess = sess
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.action_bound = action_bound
        self.learning_rate = learning_rate
        self.tau = tau
        self.batch_size = batch_size

        self.obs, self.action, self.scaled_action = self.actor_network(scope=name + "_actor")
        
        self.network_params = tf.trainable_variables(scope=name + "_actor")

        self.target_obs, self.target_action, self.target_scaled_action = self.actor_network(scope=name + "_actor_target")

        self.target_network_params = tf.trainable_variables(scope=name + "_actor_target")

        self.update_target_params = \
                                     [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +
                                                                           tf.multiply(self.target_network_params[i], 1.0 - self.tau))
                                      for i in range(len(self.target_network_params))]

        self.action_gradient = tf.placeholder(tf.float32, [None, self.action_dim])

        self.unnormalized_actor_gradients = tf.gradients(self.scaled_action, self.network_params, - self.action_gradient)
        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))

        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(self.actor_gradients, self.network_params))

        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
        
    def actor_network(self, scope):
        obs = tflearn.input_data(shape=[None, self.obs_dim])
        with tf.variable_scope(scope):
            net = tflearn.fully_connected(obs, 400, weights_init=tflearn.initializations.truncated_normal(stddev=1.0/float(self.obs_dim)))
            net = tflearn.layers.normalization.batch_normalization(net)
            net = tflearn.activations.relu(net)
            net = tflearn.fully_connected(net, 300, weights_init=tflearn.initializations.truncated_normal(stddev=1.0/400.0))
            net = tflearn.layers.normalization.batch_normalization(net)
            net = tflearn.activations.relu(net)

            w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
            action = tflearn.fully_connected(net, self.action_dim, activation='tanh', weights_init=w_init)

            scaled_action = tf.multiply(action, self.action_bound)
        return obs, action, scaled_action

    # APIs
    def train(self, obs, a_gradient):
        self.sess.run(self.optimize, feed_dict={
            self.obs: obs,
            self.action_gradient: a_gradient
        })

    def predict(self, obs):
        return self.sess.run(self.scaled_action, feed_dict={
            self.obs: obs
        })

    def predict_target(self, obs):
        # print('obs=', obs) # List of arrays.
        # print('Actor::predict_target: obs.shape=', obs.shape)
        return self.sess.run(self.target_scaled_action, feed_dict={
            self.target_obs: obs
        })

    def update_target_network(self):
        self.sess.run(self.update_target_params)
        
    def get_num_trainable_vars(self):
       return self.num_trainable_vars


# class Actor(object):
#     """
#     Input to the network is the state, output is the action
#     under a deterministic policy.
#     The output layer activation is a tanh to keep the action
#     between -action_bound and action_bound
#     """
# 
#     def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):
#         self.sess = sess
#         self.s_dim = state_dim
#         self.a_dim = action_dim
#         self.action_bound = action_bound
#         self.learning_rate = learning_rate
#         self.tau = tau
#         self.batch_size = batch_size
# 
#         # Actor Network
#         self.inputs, self.out, self.scaled_out = self.create_actor_network()
# 
#         self.network_params = tf.trainable_variables()
# 
#         # Target Network
#         self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()
# 
#         self.target_network_params = tf.trainable_variables()[
#             len(self.network_params):]
# 
#         # Op for periodically updating target network with online network
#         # weights
#         self.update_target_network_params = \
#             [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +
#                                                   tf.multiply(self.target_network_params[i], 1. - self.tau))
#                 for i in range(len(self.target_network_params))]
# 
#         # This gradient will be provided by the critic network
#         self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])
# 
#         # Combine the gradients here
#         self.unnormalized_actor_gradients = tf.gradients(
#             self.scaled_out, self.network_params, -self.action_gradient)
#         self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))
# 
#         # Optimization Op
#         self.optimize = tf.train.AdamOptimizer(self.learning_rate).\
#             apply_gradients(zip(self.actor_gradients, self.network_params))
# 
#         self.num_trainable_vars = len(
#             self.network_params) + len(self.target_network_params)
# 
#     def create_actor_network(self):
#         inputs = tflearn.input_data(shape=[None, self.s_dim])
#         net = tflearn.fully_connected(inputs, 400)
#         net = tflearn.layers.normalization.batch_normalization(net)
#         net = tflearn.activations.relu(net)
#         net = tflearn.fully_connected(net, 300)
#         net = tflearn.layers.normalization.batch_normalization(net)
#         net = tflearn.activations.relu(net)
#         # Final layer weights are init to Uniform[-3e-3, 3e-3]
#         w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
#         out = tflearn.fully_connected(
#             net, self.a_dim, activation='tanh', weights_init=w_init)
#         # Scale output to -action_bound to action_bound
#         scaled_out = tf.multiply(out, self.action_bound)
#         return inputs, out, scaled_out
# 
#     def train(self, inputs, a_gradient):
#         self.sess.run(self.optimize, feed_dict={
#             self.inputs: inputs,
#             self.action_gradient: a_gradient
#         })
# 
#     def predict(self, inputs):
#         return self.sess.run(self.scaled_out, feed_dict={
#             self.inputs: inputs
#         })
# 
#     def predict_target(self, inputs):
#         return self.sess.run(self.target_scaled_out, feed_dict={
#             self.target_inputs: inputs
#         })
# 
#     def update_target_network(self):
#         self.sess.run(self.update_target_network_params)
# 
#     def get_num_trainable_vars(self):
#         return self.num_trainable_vars

# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is
# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab
class ActorNoise:
    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):
        self.theta = theta
        self.mu = mu
        self.sigma = sigma
        self.dt = dt
        self.x0 = x0
        self.reset()

    def __call__(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \
                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)
        self.x_prev = x
        return x

    def reset(self):
        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)

    def __repr__(self):
        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)
        
